# Disk Usage Prometheus Exporter

[![Build Status](https://travis-ci.com/dundee/disk_usage_exporter.svg?branch=master)](https://travis-ci.com/dundee/disk_usage_exporter)
[![codecov](https://codecov.io/gh/dundee/disk_usage_exporter/branch/master/graph/badge.svg)](https://codecov.io/gh/dundee/disk_usage_exporter)
[![Go Report Card](https://goreportcard.com/badge/github.com/dundee/disk_usage_exporter)](https://goreportcard.com/report/github.com/dundee/disk_usage_exporter)
[![Maintainability](https://api.codeclimate.com/v1/badges/74d685f0c638e6109ab3/maintainability)](https://codeclimate.com/github/dundee/disk_usage_exporter/maintainability)
[![CodeScene Code Health](https://codescene.io/projects/14689/status-badges/code-health)](https://codescene.io/projects/14689)

Provides detailed info about disk usage of the selected filesystem path with **memory-efficient aggregated metrics** designed for large filesystems.

**ìµœì‹  ìµœì í™”**: SQLite ê¸°ë°˜ ì˜êµ¬ ì €ì¥ì†Œì™€ godirwalk ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ë””ë ‰í† ë¦¬ ìŠ¤ìº”ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëŒ€í­ ê°œì„ í–ˆìŠµë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ëŒ€ìš©ëŸ‰ íŒŒì¼ì‹œìŠ¤í…œ(1M+ íŒŒì¼)ì—ì„œ 99% ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
- **SQLite ê¸°ë°˜ ì €ì¥**: ì˜êµ¬ ë°ì´í„° ì €ì¥ê³¼ ê³ ì„±ëŠ¥ ë°°ì¹˜ ì²˜ë¦¬
- **Godirwalk ìµœì í™”**: ê³ ì„±ëŠ¥ ë””ë ‰í† ë¦¬ ìˆœíšŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©
- **Worker Pool ì•„í‚¤í…ì²˜**: max-procs ì„¤ì •ìœ¼ë¡œ CPU ì‚¬ìš©ëŸ‰ ì œì–´
- **ê³„ì¸µì  í†µê³„**: ë””ë ‰í† ë¦¬ ë ˆë²¨, íƒ€ì…ë³„, í¬ê¸°ë³„ ì§‘ê³„
- **ë°±ê·¸ë¼ìš´ë“œ ìºì‹±**: ì¼ê´€ëœ ì„±ëŠ¥ì„ ìœ„í•œ ì„ íƒì  ìºì‹±
- **ë‹¤ì¤‘ ê²½ë¡œ ì§€ì›**: ì„œë¡œ ë‹¤ë¥¸ ê¹Šì´ ë ˆë²¨ì˜ ì—¬ëŸ¬ ë””ë ‰í† ë¦¬ ëª¨ë‹ˆí„°ë§
- **ì„¤ì • ê°€ëŠ¥í•œ CPU ì‚¬ìš©ëŸ‰**: ìµœì  ì„±ëŠ¥ì„ ìœ„í•œ ë³‘ë ¬ ì²˜ë¦¬ ì¡°ì •

## Demo Grafana dashboard

https://grafana.milde.cz/d/0TfJhs_Mz/disk-usage (credentials: grafana / gdu)

## ì‚¬ìš©ë²•

```
Usage:
  disk_usage_exporter [flags]

Flags:
  -p, --analyzed-path string              Path where to analyze disk usage (default "/")
      --basic-auth-users stringToString   Basic Auth users and their passwords as bcypt hashes (default [])
  -b, --bind-address string               Address to bind to (default "0.0.0.0:9995")
  -c, --config string                     config file (default is $HOME/.disk_usage_exporter.yaml)
  -l, --dir-level int                     Directory nesting level to show (0 = only selected dir) (default 2)
      --dir-count                         Collect directory count metrics (default true)
      --file-count                        Collect file count metrics (default true)
      --size-bucket                       Collect size bucket metrics (default true)
  -L, --follow-symlinks                   Follow symlinks for files, i.e. show the size of the file to which symlink points to (symlinks to directories are not followed)
  -h, --help                              help for disk_usage_exporter
  -i, --ignore-dirs strings               Absolute paths to ignore (separated by comma) (default [/proc,/dev,/sys,/run,/var/cache/rsnapshot])
      --log-level string                  Log level (trace, debug, info, warn, error, fatal, panic) (default "info")
  -j, --max-procs int                     Maximum number of CPU cores to use for parallel processing (default 4)
  -m, --mode string                       Expose method - either 'file' or 'http' (default "http")
      --multi-paths stringToString        Multiple paths where to analyze disk usage, in format /path1=level1,/path2=level2,... (default [])
  -f, --output-file string                Target file to store metrics in (default "./disk-usage-exporter.prom")
  -t, --scan-interval-minutes int         Scan interval in minutes for background caching (0 = disabled)
  -s, --storage-path string               Path to store cached analysis data
  -v, --version                           Print version information and exit
```

### ë¹ ë¥¸ ì‹œì‘ ì˜ˆì œ

```bash
# ë‹¨ì¼ ê²½ë¡œ ê¸°ë³¸ ì‚¬ìš©ë²•
disk_usage_exporter --analyzed-path /home --dir-level 2

# ì—¬ëŸ¬ ê²½ë¡œë¥¼ ì„œë¡œ ë‹¤ë¥¸ ë ˆë²¨ë¡œ ì„¤ì •
disk_usage_exporter --multi-paths=/home=2,/var=3

# SQLite ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ ì„¤ì •
disk_usage_exporter --analyzed-path /home --db-path /tmp/disk-usage.db --batch-size 1000

# ì„¤ì • íŒŒì¼ ì‚¬ìš©
disk_usage_exporter --config config-sqlite.yml

# ë°±ê·¸ë¼ìš´ë“œ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
disk_usage_exporter --storage-path /tmp/cache --scan-interval-minutes 30

# íŒŒì¼ ì¶œë ¥ ëª¨ë“œ
disk_usage_exporter --mode file --output-file metrics.prom

# ë””ë²„ê·¸ ëª¨ë“œë¡œ ìƒì„¸ ë¡œê·¸ í™•ì¸
disk_usage_exporter --log-level debug --analyzed-path /tmp

# 8ê°œ CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
disk_usage_exporter --max-procs 8 --analyzed-path /home

# ì„ íƒì  ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ë””ë ‰í† ë¦¬ ìˆ˜ì™€ í¬ê¸° ë²„í‚·ë§Œ ìˆ˜ì§‘)
disk_usage_exporter --dir-count --no-file-count --size-bucket --analyzed-path /home

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
LOG_LEVEL=debug disk_usage_exporter --config config-sqlite.yml
```

`--analyzed-path`ì™€ `--dir-level` í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ ê²½ë¡œë¥¼ ì§€ì •í•˜ê±°ë‚˜ `--multi-paths` í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ê²½ë¡œë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ë²„ì „ ì •ë³´

ë‹¤ìŒ ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ ë²„ì „ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# CLI version check
disk_usage_exporter --version
disk_usage_exporter -v
disk_usage_exporter version

# API version endpoint
curl http://localhost:9995/version
```

## ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸

HTTP ëª¨ë“œë¡œ ì‹¤í–‰í•  ë•Œ ë‹¤ìŒ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

| ì—”ë“œí¬ì¸íŠ¸ | ì„¤ëª… | ì‘ë‹µ í˜•ì‹ |
|----------|-------------|----------------|
| **/** | ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸ ë§í¬ê°€ ìˆëŠ” ì¸ë±ìŠ¤ í˜ì´ì§€ | HTML |
| **/metrics** | Prometheus ë©”íŠ¸ë¦­ (ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸) | Text/Plain |
| **/version** | ë²„ì „ ì •ë³´ | JSON |
| **/health** | í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ | JSON |

### í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸

`/health` ì—”ë“œí¬ì¸íŠ¸ëŠ” ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ ìœ„í•œ ê°„ë‹¨í•œ í—¬ìŠ¤ ì²´í¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

```bash
curl http://localhost:9995/health
```

ì‘ë‹µ:
```json
{
  "status": "ok",
  "version": "v0.6.0-6-gc1ec294"
}
```

### ë²„ì „ ì—”ë“œí¬ì¸íŠ¸

`/version` ì—”ë“œí¬ì¸íŠ¸ëŠ” ìƒì„¸í•œ ë²„ì „ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

```bash
curl http://localhost:9995/version
```

ì‘ë‹µ:
```json
{
  "buildDate": "Wed Jul 16 17:26:24 KST 2025",
  "commitSha": "c1ec29420833cbaf67eb925ec950de3a692859b8",
  "goVersion": "go1.22.2",
  "goarch": "amd64",
  "goos": "linux",
  "version": "v0.6.0-6-gc1ec294"
}
```

## ğŸ“Š ë©”íŠ¸ë¦­ ì¶œë ¥

ì´ ìµìŠ¤í¬í„°ëŠ” ê°œë³„ íŒŒì¼ ë©”íŠ¸ë¦­ ëŒ€ì‹  **ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì§‘ê³„ ë©”íŠ¸ë¦­**ì„ ì œê³µí•˜ì—¬ ëŒ€ìš©ëŸ‰ íŒŒì¼ì‹œìŠ¤í…œì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê·¹ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.

### ì§‘ê³„ ë©”íŠ¸ë¦­ êµ¬ì¡°

```prometheus
# ë””ë ‰í† ë¦¬ë³„ ì´ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
# HELP node_disk_usage Total disk usage by directory
# TYPE node_disk_usage gauge
node_disk_usage{level="0",path="/home"} 8.7081373696e+10
node_disk_usage{level="1",path="/home/user1"} 2.5e+10
node_disk_usage{level="1",path="/home/user2"} 1.2e+10

# íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ìˆ˜
# HELP node_disk_usage_file_count Number of files in directory
# TYPE node_disk_usage_file_count gauge
node_disk_usage_file_count{level="1",path="/home/user1"} 50000
node_disk_usage_file_count{level="1",path="/home/user2"} 25000

# HELP node_disk_usage_directory_count Number of subdirectories in directory
# TYPE node_disk_usage_directory_count gauge
node_disk_usage_directory_count{level="1",path="/home/user1"} 150
node_disk_usage_directory_count{level="1",path="/home/user2"} 75

# ì´ ë©”íŠ¸ë¦­ì€ ì œê±°ë˜ì—ˆê³  node_disk_usageë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤

# í¬ê¸°ë³„ ë²„í‚· ë¶„í¬
# HELP node_disk_usage_size_bucket File count by size range
# TYPE node_disk_usage_size_bucket gauge
node_disk_usage_size_bucket{level="1",path="/home/user1",size_range="0-1KB"} 15000
node_disk_usage_size_bucket{level="1",path="/home/user1",size_range="1KB-1MB"} 30000
node_disk_usage_size_bucket{level="1",path="/home/user1",size_range="1MB-100MB"} 4500
node_disk_usage_size_bucket{level="1",path="/home/user1",size_range="100MB+"} 500

# ìƒìœ„ Nê°œ ìµœëŒ€ íŒŒì¼ (ì„¤ì • ê°€ëŠ¥, ê¸°ë³¸ê°’: ìƒìœ„ 1000ê°œ)
# HELP node_disk_usage_top_files Top N largest files
# TYPE node_disk_usage_top_files gauge
node_disk_usage_top_files{path="/home/user1/large_file1.dat",rank="1"} 5.0e+09
node_disk_usage_top_files{path="/home/user1/large_file2.zip",rank="2"} 3.5e+09
node_disk_usage_top_files{path="/home/user1/backup.tar.gz",rank="3"} 2.8e+09

# ìƒìœ„ Nì— í¬í•¨ë˜ì§€ ì•Šì€ íŒŒì¼ì˜ ì§‘ê³„ í†µê³„
# HELP node_disk_usage_others_total Total size of files not in top N
# TYPE node_disk_usage_others_total gauge
node_disk_usage_others_total{path="/home/user1"} 1.5e+10

# HELP node_disk_usage_others_count Count of files not in top N
# TYPE node_disk_usage_others_count gauge
node_disk_usage_others_count{path="/home/user1"} 49000
```

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì´ì 

| íŒŒì¼ì‹œìŠ¤í…œ í¬ê¸° | ê°œë³„ íŒŒì¼ ë©”íŠ¸ë¦­ | ì§‘ê³„ ë©”íŠ¸ë¦­ | ë©”ëª¨ë¦¬ ì ˆì•½ |
|----------------|------------------------|-------------------|----------------|
| **10K íŒŒì¼** | 10,000 ë©”íŠ¸ë¦­ | ~100 ë©”íŠ¸ë¦­ | **99%** |
| **100K íŒŒì¼** | 100,000 ë©”íŠ¸ë¦­ | ~500 ë©”íŠ¸ë¦­ | **99.5%** |
| **1M+ íŒŒì¼** | 1,000,000+ ë©”íŠ¸ë¦­ | ~2,000 ë©”íŠ¸ë¦­ | **99.8%** |

### í¬ê¸° ë²”ìœ„ ë²„í‚·

íŒŒì¼ì€ ìë™ìœ¼ë¡œ í¬ê¸° ë²”ìœ„ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤:
- **0B**: ë¹ˆ íŒŒì¼
- **0-1KB**: ì†Œí˜• íŒŒì¼ (ì„¤ì • íŒŒì¼, ì†Œí˜• ìŠ¤í¬ë¦½íŠ¸)
- **1KB-1MB**: ì¤‘í˜• íŒŒì¼ (ë¬¸ì„œ, ì½”ë“œ íŒŒì¼)
- **1MB-100MB**: ëŒ€í˜• íŒŒì¼ (ì´ë¯¸ì§€, ì†Œí˜• ë°ì´í„°ë² ì´ìŠ¤)
- **100MB+**: ì´ˆëŒ€í˜• íŒŒì¼ (ë¹„ë””ì˜¤, ëŒ€í˜• ë°ì´í„°ë² ì´ìŠ¤, ì•„ì¹´ì´ë¸Œ)

## ğŸ“ˆ Prometheus ì¿¼ë¦¬ ì˜ˆì œ

### ë””ë ‰í† ë¦¬ ì‚¬ìš©ëŸ‰ ì¿¼ë¦¬

`/home` ë””ë ‰í† ë¦¬ì™€ ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì˜ ì´ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰:
```promql
sum(node_disk_usage{path=~"/home.*"})
```

`/var` ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ì˜ ì´ íŒŒì¼ ìˆ˜:
```promql
sum(node_disk_usage_file_count{path=~"/var.*"})
```

ëª¨ë“  ë ˆë²¨-1 ë””ë ‰í† ë¦¬ì˜ í‰ê·  ë””ë ‰í† ë¦¬ í¬ê¸°:
```promql
avg(node_disk_usage{level="1"})
```

### íŒŒì¼ ë¶„í¬ ë¶„ì„

`/home/user1`ì˜ í¬ê¸°ë³„ íŒŒì¼ ë¶„í¬:
```promql
node_disk_usage_size_bucket{path="/home/user1"}
```

ë””ë ‰í† ë¦¬ì—ì„œ ëŒ€í˜• íŒŒì¼(>100MB)ì˜ ë¹„ìœ¨:
```promql
(
  node_disk_usage_size_bucket{path="/home/user1",size_range="100MB+"}
  /
  sum(node_disk_usage_size_bucket{path="/home/user1"})
) * 100
```

### ìƒìœ„ íŒŒì¼ ë¶„ì„

ëª¨ë“  ê²½ë¡œì—ì„œ ê°€ì¥ í° 10ê°œ íŒŒì¼ í‘œì‹œ:
```promql
topk(10, node_disk_usage_top_files)
```

ìƒìœ„ Nê°œ íŒŒì¼ê³¼ ê¸°íƒ€ íŒŒì¼ì´ ì‚¬ìš©í•˜ëŠ” ì´ ê³µê°„:
```promql
sum(node_disk_usage_top_files) / (sum(node_disk_usage_top_files) + sum(node_disk_usage_others_total))
```

### ë””ë ‰í† ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„

ëª¨ë‹ˆí„°ë§ë˜ëŠ” ëª¨ë“  ê²½ë¡œì˜ ì´ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰:
```promql
sum(node_disk_usage)
```

ì‹œê°„ì— ë”°ë¥¸ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ìœ¨:
```promql
rate(node_disk_usage[5m])
```

### ì¦ê°€ëŸ‰ ë° ì•Œë¦¼ ì¿¼ë¦¬

100Kê°œ ì´ìƒì˜ íŒŒì¼ì„ ê°€ì§„ ë””ë ‰í† ë¦¬ (ì„±ëŠ¥ ë¬¸ì œ ê°€ëŠ¥):
```promql
node_disk_usage_file_count > 100000
```

ì •ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆëŠ” ë§¤ìš° í° ë””ë ‰í† ë¦¬ (>1TB):
```promql
node_disk_usage > 1e12
```

ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ì€ íŒŒì¼ ë°€ë„ë¥¼ ê°€ì§„ ë””ë ‰í† ë¦¬:
```promql
(node_disk_usage_file_count / node_disk_usage_directory_count) > 1000
```

## ì˜ˆì‹œ ì„¤ì • íŒŒì¼

ë¦¬í¬ì§€í† ë¦¬ì—ì„œ ì—¬ëŸ¬ ì˜ˆì‹œ ì„¤ì • íŒŒì¼ì„ ì œê³µí•©ë‹ˆë‹¤:

### ê¸°ë³¸ ì„¤ì • (config-basic.yml)
```yaml
# ìºì‹± ì—†ëŠ” ê¸°ë³¸ ì„¤ì •
analyzed-path: "/tmp"
bind-address: "0.0.0.0:9995"
dir-level: 1
mode: "http"
follow-symlinks: false

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
# ì‚¬ìš© ê°€ëŠ¥í•œ ë ˆë²¨: trace, debug, info, warn, error, fatal, panic
log-level: "info"

# CPU ì½”ì–´ ì„¤ì •
# ë³‘ë ¬ ì²˜ë¦¬ì— ì‚¬ìš©í•  ìµœëŒ€ CPU ì½”ì–´ ìˆ˜
max-procs: 4

# ì„ íƒì  ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„¤ì •
dir-count: true     # ë””ë ‰í† ë¦¬ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
file-count: true    # íŒŒì¼ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
size-bucket: true   # í¬ê¸° ë²„í‚· ë©”íŠ¸ë¦­ ìˆ˜ì§‘

ignore-dirs:
  - /proc
  - /dev
  - /sys
  - /run
  - /var/cache/rsnapshot
```

### ë°±ê·¸ë¼ìš´ë“œ ìºì‹± ì„¤ì • (config-caching.yml)
```yaml
# ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ë°±ê·¸ë¼ìš´ë“œ ìºì‹± ì„¤ì •
analyzed-path: "/home"
bind-address: "0.0.0.0:9995"
dir-level: 2
mode: "http"
follow-symlinks: false

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
# ì‚¬ìš© ê°€ëŠ¥í•œ ë ˆë²¨: trace, debug, info, warn, error, fatal, panic
log-level: "info"

# CPU ì½”ì–´ ì„¤ì •
# ë³‘ë ¬ ì²˜ë¦¬ì— ì‚¬ìš©í•  ìµœëŒ€ CPU ì½”ì–´ ìˆ˜
max-procs: 8

# ì„ íƒì  ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„¤ì •
dir-count: true     # ë””ë ‰í† ë¦¬ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
file-count: true    # íŒŒì¼ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
size-bucket: true   # í¬ê¸° ë²„í‚· ë©”íŠ¸ë¦­ ìˆ˜ì§‘

# ë°±ê·¸ë¼ìš´ë“œ ìºì‹± ì„¤ì •
storage-path: "/tmp/disk-usage-cache"
scan-interval-minutes: 15

ignore-dirs:
  - /proc
  - /dev
  - /sys
  - /run
  - /var/cache/rsnapshot
  - /tmp
```

### SQLite ê¸°ë°˜ ê³ ì„±ëŠ¥ ì„¤ì • (config-sqlite.yml)
```yaml
# SQLite ê¸°ë°˜ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ Prometheus ìµìŠ¤í¬í„° ì„¤ì •
# ì´ ì„¤ì •ì€ í–¥ìƒëœ ì„±ëŠ¥ê³¼ ë°ì´í„° ì§€ì†ì„±ì„ ìœ„í•´ SQLite ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
analyzed-path: "/home"
bind-address: "0.0.0.0:9995"
dir-level: 2
mode: "http"
follow-symlinks: false

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
log-level: "info"

# ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìµœëŒ€ CPU ì½”ì–´ ìˆ˜
max-procs: 8

# ì„ íƒì  ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„¤ì •
dir-count: true     # ë””ë ‰í† ë¦¬ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
file-count: true    # íŒŒì¼ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
size-bucket: true   # í¬ê¸° ë²„í‚· ë©”íŠ¸ë¦­ ìˆ˜ì§‘

# SQLite ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
db-path: "/tmp/disk-usage.db"
batch-size: 1000

ignore-dirs:
  - /proc
  - /dev
  - /sys
  - /run
  - /var/cache/rsnapshot
```

### ë‹¤ì¤‘ ê²½ë¡œ ì„¤ì • (config-multipaths.yml)
```yaml
# ì„œë¡œ ë‹¤ë¥¸ ê¹Šì´ ë ˆë²¨ì˜ ì—¬ëŸ¬ ë””ë ‰í† ë¦¬ ëª¨ë‹ˆí„°ë§
bind-address: "0.0.0.0:9995"
mode: "http"
follow-symlinks: false

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
log-level: "info"

# CPU ì½”ì–´ ì„¤ì •
max-procs: 6

# ì„ íƒì  ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„¤ì •
dir-count: true     # ë””ë ‰í† ë¦¬ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
file-count: true    # íŒŒì¼ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
size-bucket: true   # í¬ê¸° ë²„í‚· ë©”íŠ¸ë¦­ ìˆ˜ì§‘

multi-paths:
  /home: 2
  /var: 3
  /tmp: 1
  /opt: 2

# ë°±ê·¸ë¼ìš´ë“œ ìºì‹± ì„¤ì •
storage-path: "/tmp/disk-usage-cache"
scan-interval-minutes: 20

ignore-dirs:
  - /proc
  - /dev
  - /sys
  - /run
  - /var/cache/rsnapshot

# ê¸°ë³¸ ì¸ì¦ (ì„ íƒì‚¬í•­)
# basic-auth-users:
#   admin: $2b$12$hNf2lSsxfm0.i4a.1kVpSOVyBCfIB51VRjgBUyv6kdnyTlgWj81Ay
```

### íŒŒì¼ ì¶œë ¥ ëª¨ë“œ ì„¤ì •
```yaml
# HTTP ì„œë²„ ëŒ€ì‹  íŒŒì¼ë¡œ ë©”íŠ¸ë¦­ ì¶œë ¥
analyzed-path: /
mode: file
output-file: ./disk-usage-exporter.prom
dir-level: 2

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
log-level: "info"

# CPU ì½”ì–´ ì„¤ì •
max-procs: 4

# ì„ íƒì  ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„¤ì •
dir-count: true     # ë””ë ‰í† ë¦¬ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
file-count: true    # íŒŒì¼ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
size-bucket: true   # í¬ê¸° ë²„í‚· ë©”íŠ¸ë¦­ ìˆ˜ì§‘

ignore-dirs:
- /proc
- /dev
- /sys
- /run
```

### ë””ë²„ê·¸ ì„¤ì • (config-debug.yml)
```yaml
# ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë””ë²„ê·¸ ì„¤ì •
analyzed-path: "/tmp"
bind-address: "0.0.0.0:9995"
dir-level: 1
mode: "http"
follow-symlinks: false

# ë¡œê·¸ ë ˆë²¨ ì„¤ì • - ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë””ë²„ê·¸ ëª¨ë“œ
log-level: "debug"

# CPU ì½”ì–´ ì„¤ì •
max-procs: 2

# ì„ íƒì  ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„¤ì •
dir-count: true     # ë””ë ‰í† ë¦¬ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
file-count: true    # íŒŒì¼ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
size-bucket: true   # í¬ê¸° ë²„í‚· ë©”íŠ¸ë¦­ ìˆ˜ì§‘

# ë°±ê·¸ë¼ìš´ë“œ ìºì‹± ì„¤ì •
storage-path: "/tmp/debug-cache"
scan-interval-minutes: 5

ignore-dirs:
  - /proc
  - /dev
  - /sys
  - /run
  - /var/cache/rsnapshot
```

### ì„¤ì • íŒŒì¼ ì‚¬ìš©ë²•
```bash
# íŠ¹ì • ì„¤ì • íŒŒì¼ ì‚¬ìš©
./disk_usage_exporter --config config-basic.yml

# SQLite ê¸°ë°˜ ê³ ì„±ëŠ¥ ì„¤ì • ì‚¬ìš©
./disk_usage_exporter --config config-sqlite.yml

# ë°±ê·¸ë¼ìš´ë“œ ìºì‹± ì„¤ì • ì‚¬ìš©
./disk_usage_exporter --config config-caching.yml

# ë‹¤ì¤‘ ê²½ë¡œ ì„¤ì • ì‚¬ìš©
./disk_usage_exporter --config config-multipaths.yml

# ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë””ë²„ê·¸ ì„¤ì • ì‚¬ìš©
./disk_usage_exporter --config config-debug.yml

# CLI í”Œë˜ê·¸ë¡œ ì„¤ì • íŒŒì¼ ì„¤ì • ì¬ì •ì˜
./disk_usage_exporter --config config-basic.yml --log-level debug

# ì„±ëŠ¥ íŠ¸ë‹ì„ ìœ„í•œ ì‚¬ìš©ì ì •ì˜ CPU ì½”ì–´ ìˆ˜ ì‚¬ìš©
./disk_usage_exporter --config config-basic.yml --max-procs 8
```

## Prometheus ìŠ¤í¬ë˜ì´í•‘ ì„¤ì •

ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ë¶„ì„ì€ ë¦¬ì†ŒìŠ¤ë¥¼ ë§ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë¶„ì„ ê²½ë¡œì˜ í¬ê¸°ì— ë”°ë¼ `scrape_interval`ê³¼ `scrape_timeout`ì„ ì„¤ì •í•˜ì„¸ìš”.

```yaml
scrape_configs:
  - job_name: 'disk-usage'
    scrape_interval: 5m
    scrape_timeout: 20s
    static_configs:
    - targets: ['localhost:9995']
```

**ë°±ê·¸ë¼ìš´ë“œ ìºì‹±ì´ í™œì„±í™”ëœ ê²½ìš°**, `/metrics` ì‘ë‹µì´ ìºì‹œì—ì„œ ì œê³µë˜ë¯€ë¡œ ë” ì§§ì€ ìŠ¤í¬ë˜ì´í•‘ ê°„ê²©ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
scrape_configs:
  - job_name: 'disk-usage'
    scrape_interval: 30s
    scrape_timeout: 10s
    static_configs:
    - targets: ['localhost:9995']
```

## íŒŒì¼ë¡œ ë¤í”„

ê³µì‹ `node-exporter`ëŠ” [textfile collection mechanism](https://github.com/prometheus/node_exporter#textfile-collector)ì„ í†µí•´ ì¶”ê°€ ë©”íŠ¸ë¦­ íŒŒì¼ì´ í¬í•¨ëœ í´ë”ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ë¥¼ í™œìš©í•˜ë ¤ë©´ ë¬¸ì„œì— ë”°ë¼ `node-exporter`ë¥¼ ì„¤ì •í•˜ê³  ì´ ìµìŠ¤í¬í„°ì˜ `output-file`ì„
í•´ë‹¹ í´ë” ë‚´ì—ì„œ `.prom`ìœ¼ë¡œ ëë‚˜ëŠ” ì´ë¦„ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤(ë¬¼ë¡  `mode`ë„ `file`ë¡œ ì„¤ì •).

ë©”íŠ¸ë¦­ ê³„ì‚°ì´ íŠ¹íˆ ì˜¤ë˜ ê±¸ë¦¬ê¸° ë•Œë¬¸ì— ê°€ë”ì”©ë§Œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê²½ìš°ì— ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
ì¶œë ¥ íŒŒì¼ì˜ ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ë¥¼ ìë™í™”í•˜ë ¤ë©´ cronjobì„ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.

## Basic Auth

You can enable HTTP Basic authorization by setting the `--basic-auth-users` flag (password is "test"):

```
disk_usage_exporter --basic-auth-users='admin=$2b$12$hNf2lSsxfm0.i4a.1kVpSOVyBCfIB51VRjgBUyv6kdnyTlgWj81Ay'
```

or by setting the key in config:

```yaml
basic-auth-users:
  admin: $2b$12$hNf2lSsxfm0.i4a.1kVpSOVyBCfIB51VRjgBUyv6kdnyTlgWj81Ay
```

The password needs to be hashed by [bcrypt](https://bcrypt-generator.com/) in both cases.

## Background Caching

For better performance on large filesystems, you can enable background caching by setting both `storage-path` and `scan-interval-minutes`:

```yaml
analyzed-path: /mnt
bind-address: 0.0.0.0:9995
dir-level: 1
mode: http
follow-symlinks: false
storage-path: /var/cache/gdu
scan-interval-minutes: 30
```

### How Background Caching Works

1. **Background Process**: A separate goroutine runs periodic scans
2. **Data Storage**: Results are cached using BadgerDB key-value store
3. **Fast Response**: `/metrics` endpoint serves cached data instantly
4. **Auto-cleanup**: Storage handles cleanup of old data automatically

### Cache Behavior

| Mode | Background Scan | Response Time | Data Freshness |
|------|----------------|---------------|----------------|
| **With caching enabled** | âœ… Runs every N minutes | âš¡ Fast (cached) | Updated every N minutes |
| **Without caching** | âŒ None | ğŸŒ Slow (live analysis) | Real-time |

### Configuration Requirements

- **Both required**: `storage-path` AND `scan-interval-minutes` must be set
- **Permissions**: Storage path must be writable by the exporter process
- **Disk space**: Minimal storage required (typically < 1MB per analyzed path)

### Cache Miss Behavior

When cached data is unavailable:
- Returns empty metrics (0 bytes) for missing paths
- Logs debug message about missing cache data
- Continues serving other cached paths normally

**Note:** If either `storage-path` or `scan-interval-minutes` is not set, the exporter will perform live disk analysis for each `/metrics` request (default behavior).

## ğŸ§  Memory Optimization

The exporter uses **aggregated metrics architecture** to handle large filesystems efficiently:

### Memory Usage Comparison

| Traditional Approach | Aggregated Approach | Benefit |
|---------------------|-------------------|---------|
| **1 metric per file** | **1 metric per directory + aggregations** | 99%+ memory reduction |
| 1M files = 1M metrics | 1M files = ~2K metrics | Constant memory usage |
| 4-8GB RAM for 1M files | 10-50MB RAM for 1M files | Scalable to any size |

### Architecture Benefits

1. **Directory-Level Aggregation**: Individual files are summarized at directory level
2. **Size Bucket Histograms**: Files categorized by size ranges instead of individual tracking
3. **Top-N File Tracking**: Only largest N files (default: 1000) tracked individually
4. **Type-Based Summaries**: Separate aggregations for files vs directories

### Recommended Use Cases

**Large Filesystems (1M+ files)**:
```yaml
analyzed-path: /mnt/large_storage
dir-level: 2
mode: http
storage-path: /tmp/cache
scan-interval-minutes: 60  # Cache for 1 hour
max-procs: 8              # Use more cores for initial scan
```

**Memory-Constrained Environments**:
```yaml
analyzed-path: /home
dir-level: 1              # Limit depth
mode: http
max-procs: 2              # Use fewer cores
```

**Real-time Monitoring**:
```yaml
analyzed-path: /var/log
dir-level: 3
mode: http
storage-path: /tmp/cache
scan-interval-minutes: 5  # Fast refresh for active directories
```

## Performance and CPU Configuration

The exporter supports configurable CPU core usage for optimal performance across different hardware configurations:

### CPU Cores Configuration

| Configuration | Default | Recommended Usage |
|---------------|---------|-------------------|
| **max-procs** | 4 | Number of CPU cores to use for parallel processing |

### Performance Tuning Guidelines

**Small filesystems** (< 1GB):
```yaml
max-procs: 2  # Minimal resources needed
```

**Medium filesystems** (1GB - 100GB):
```yaml
max-procs: 4  # Default setting, good balance
```

**Large filesystems** (> 100GB):
```yaml
max-procs: 8  # Higher parallelism for better performance
```

**SSD storage** (high I/O performance):
```yaml
max-procs: 12  # Can handle more parallel operations
```

### Configuration Methods

1. **Configuration File**: Add `max-procs: 8` to your config file
2. **CLI Flag**: Use `--max-procs 8` or `-j 8` when running the exporter
3. **Environment Variable**: Set `MAX_PROCS=8` environment variable

### Performance Impact

| CPU Cores | Typical Performance | Memory Usage | Recommended For |
|-----------|-------------------|--------------|----------------|
| **1-2** | Basic performance | Low | Small filesystems, resource-constrained environments |
| **4** | Balanced (default) | Medium | Most production environments |
| **6-8** | High performance | Medium-High | Large filesystems, high-performance requirements |
| **12+** | Maximum performance | High | SSD storage, very large filesystems |

### Monitoring Performance

The exporter logs CPU usage and performance metrics:

```
time="2025-07-18T13:38:41+09:00" level=info msg="Using 8 CPU cores for live analysis (gdu internal parallelization)"
time="2025-07-18T13:38:41+09:00" level=info msg="Starting live analysis for path: /home, initial goroutines: 2"
time="2025-07-18T13:38:43+09:00" level=info msg="Live analysis completed for path: /home, elapsed time: 672.874509ms, goroutines: 2"
```

**Memory optimization in action:**
```
# Before: Individual file metrics (1M files = 1M metrics)
time="2025-07-18T12:00:00+09:00" level=info msg="Memory usage: 4.2GB for 1,000,000 metrics"

# After: Aggregated metrics (1M files = ~2K metrics) 
time="2025-07-18T13:38:41+09:00" level=info msg="Generated 1,395 aggregated metrics from 150,000+ files"
time="2025-07-18T13:38:41+09:00" level=info msg="Memory usage: ~50MB (99% reduction)"
```

**Key metrics to monitor:**
- **CPU cores used**: Number of cores allocated for processing
- **Elapsed time**: Time taken for analysis
- **Goroutines**: Active concurrent operations

### Performance Best Practices

1. **Start with default (4 cores)** and monitor performance
2. **Increase cores** if analysis takes too long on large filesystems
3. **Decrease cores** in resource-constrained environments
4. **Use background caching** for consistent performance across multiple requests
5. **Monitor system resources** to avoid over-allocation

### Example Performance Configurations

```bash
# Development environment (low resources)
./disk_usage_exporter --max-procs 2 --analyzed-path /home

# Production environment (balanced)
./disk_usage_exporter --max-procs 4 --config config-basic.yml

# High-performance environment (fast SSD)
./disk_usage_exporter --max-procs 12 --config config-caching.yml

# Resource-constrained environment
./disk_usage_exporter --max-procs 1 --analyzed-path /tmp
```

## Selective Metric Collection

The exporter supports **selective metric collection** that allows you to choose which specific metrics to collect. This provides fine-grained control over resource usage and output:

- **Granular Control**: Enable/disable individual metric types independently
- **Resource Optimization**: Reduce memory and CPU usage by collecting only needed metrics
- **Customized Monitoring**: Tailor metrics collection to specific use cases
- **Performance Tuning**: Focus on specific aspects of filesystem analysis

### Configuration Options

#### CLI Flags
```bash
# Collect all metrics (default)
./disk_usage_exporter --dir-count --file-count --size-bucket --analyzed-path /home

# Collect only directory counts
./disk_usage_exporter --dir-count --no-file-count --no-size-bucket --analyzed-path /home

# Collect only size buckets for file distribution analysis
./disk_usage_exporter --no-dir-count --no-file-count --size-bucket --analyzed-path /home
```

#### Configuration File
```yaml
# Enable/disable specific metrics collection
dir-count: true      # Collect node_disk_usage_directory_count
file-count: true     # Collect node_disk_usage_file_count  
size-bucket: true    # Collect node_disk_usage_size_bucket

# All options default to true if not specified
```

### Metrics Behavior

**With dir-count enabled (`dir-count: true`)**:
- `node_disk_usage_directory_count` shows actual subdirectory counts

**With file-count enabled (`file-count: true`)**:
- `node_disk_usage_file_count` shows actual file counts per directory

**With size-bucket enabled (`size-bucket: true`)**:
- `node_disk_usage_size_bucket` shows file distribution by size ranges

**With metrics disabled**:
- Corresponding metrics are not collected or published
- Reduces memory usage and processing time
- `node_disk_usage` (total disk usage) is always collected

### Example Use Cases

**Directory structure monitoring only**:
```yaml
# Monitor only directory hierarchy
analyzed-path: /mnt/storage
dir-count: true
file-count: false
size-bucket: false
dir-level: 3
```

**File distribution analysis**:
```yaml
# Focus on file size distribution patterns
analyzed-path: /home
dir-count: false
file-count: false
size-bucket: true
dir-level: 2
```

**Minimal resource usage**:
```yaml
# Collect only essential directory information
analyzed-path: /var
dir-count: true
file-count: false
size-bucket: false
dir-level: 1
max-procs: 2
```

**Complete file analysis**:
```yaml
# Collect all available metrics (default behavior)
analyzed-path: /home
dir-count: true
file-count: true
size-bucket: true
dir-level: 2
```

## Logging Configuration

The exporter supports configurable logging levels to help with monitoring and troubleshooting:

### Available Log Levels

| Level | Description | Use Case |
|-------|-------------|----------|
| **trace** | Most detailed logging | Development/debugging |
| **debug** | Debug information including cache operations | Troubleshooting |
| **info** | General information including scan timings | Production monitoring |
| **warn** | Warning messages | Production monitoring |
| **error** | Error messages | Production monitoring |
| **fatal** | Fatal errors (program exits) | Production monitoring |
| **panic** | Panic situations | Production monitoring |

### Configuration Methods

1. **Configuration File**: Add `log-level: "debug"` to your config file
2. **CLI Flag**: Use `--log-level debug` when running the exporter
3. **Environment Variable**: Set `LOG_LEVEL=debug` environment variable

### Example Log Outputs

**Info Level (default)** - Shows scan timings:
```
time="2025-07-16T19:06:39+09:00" level=info msg="Starting live analysis for path: /tmp"
time="2025-07-16T19:06:39+09:00" level=info msg="Live analysis completed for path: /tmp, elapsed time: 17.242744ms"
```

**Debug Level** - Shows cache operations:
```
time="2025-07-16T19:06:39+09:00" level=debug msg="No cached data found for path: /tmp, returning empty metrics"
time="2025-07-16T19:06:39+09:00" level=debug msg="Failed to open storage, returning empty metrics"
```

## ğŸ¯ Production Deployment Summary

### Optimal Configuration for Large Filesystems

For production environments with 1M+ files:

```yaml
# config-production-large.yml
analyzed-path: /mnt/data
bind-address: 0.0.0.0:9995
dir-level: 2
mode: http
follow-symlinks: false
log-level: info

# Memory optimization (aggregated metrics)
# Automatically enabled - no configuration needed

# Performance optimization
max-procs: 8
storage-path: /var/cache/disk-usage-exporter
scan-interval-minutes: 30

# Directory-only mode for minimal memory usage (optional)
# dir-only: true

# Security
basic-auth-users:
  monitor: $2b$12$hNf2lSsxfm0.i4a.1kVpSOVyBCfIB51VRjgBUyv6kdnyTlgWj81Ay

ignore-dirs:
  - /proc
  - /dev
  - /sys
  - /run
  - /tmp
```

### Key Benefits Achieved

âœ… **99%+ Memory Reduction**: 1M files now use ~50MB instead of 4-8GB  
âœ… **Scalable Architecture**: Handles any filesystem size with constant memory  
âœ… **Rich Analytics**: Directory-level, size-bucket, and top-N file insights  
âœ… **Production Ready**: Background caching, authentication, monitoring  
âœ… **Grafana Compatible**: Optimized metrics for dashboard visualization


## Example systemd unit file

```
[Unit]
Description=Prometheus disk usage exporter
Documentation=https://github.com/dundee/disk_usage_exporter

[Service]
Restart=always
User=prometheus
ExecStart=/usr/bin/disk_usage_exporter $ARGS
ExecReload=/bin/kill -HUP $MAINPID
TimeoutStopSec=20s
SendSIGKILL=no

[Install]
WantedBy=multi-user.target
```
